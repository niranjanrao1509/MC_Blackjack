{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.style\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In model free prediction we do not have knowledge of the dynamic of the MDP, reward function etc. Given a policy we want to estimate value function of that policy.\n",
    "\n",
    "### Monte Carlo learning is used in episodic MDP's i.e it learns from cmoplete episodes. The value function is estimated by sampling returns of being in a state and following policy 'PI' over many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    1) Initialize a random policy 'PI'<------ To be evaluated\n",
    "    2) Initialize arbitray state value function v('PI') under the policy\n",
    "    3) Generate Episodes under policy 'PI' store in an array. An episode is a tuple of <state,action,reward>\n",
    "    #NOTE: Action is chosen such that it follows policy 'PI'\n",
    "    4) For each episode increment N(s)<---- N(s)+1 for every first visit to state s for each episode.\n",
    "    5) Calculate total return for each episode starting from state s S(s)<----- S(s)+G_t\n",
    "    #This is return following first occurance of state s in each episode\n",
    "    6) Average these returns\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final value function\n",
    "    V = defaultdict(float)\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        #STEP 3 Generating episodes under policy 'PI'\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "    \n",
    "        states_in_episode = set([tuple(x[0]) for x in episode])\n",
    "        for state in states_in_episode:\n",
    "            # STEP 4: Find the first occurance of the state in each episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == state)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[state] += G\n",
    "            returns_count[state] += 1.0\n",
    "            V[state] = returns_sum[state] / returns_count[state]\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is >= 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    return 0 if score >= 20 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement is done by making policy greedy wrt to estimated action value function that is obtained by Policy Evaluation Step. We store the Q value in each state during policy evaluation step, which gives us the value function of taking every possible action from that state. Which is then used to find the greedy action that maximum Q value possible from that state. And policy is changed by taking that action which maximizes the Q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=500000, epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'plot_value_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d069d52a8eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0maction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_value_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Optimal Value Function\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'plot_value_function'"
     ]
    }
   ],
   "source": [
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plt.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/10000."
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'plot_value_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8adf16e51f78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mV_10k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_value_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV_10k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"10,000 Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mV_500k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_value_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV_500k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"500,000 Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'plot_value_function'"
     ]
    }
   ],
   "source": [
    "V_10k = mc_prediction(sample_policy, env, num_episodes=10000)\n",
    "plt.plot_value_function(V_10k, title=\"10,000 Steps\")\n",
    "\n",
    "V_500k = mc_prediction(sample_policy, env, num_episodes=500000)\n",
    "plt.plot_value_function(V_500k, title=\"500,000 Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random policy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2, False)\n",
      "Stick\n",
      "Reward -1.0\n",
      "You lost\n",
      "(12, 8, False)\n",
      "hit\n",
      "(21, 8, False)\n",
      "Stick\n",
      "Reward 1.0\n",
      "You won\n",
      "(17, 10, False)\n",
      "hit\n",
      "Reward -1\n",
      "You lost\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(5):\n",
    "    state=env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        print('Stick') if action == 0 else print('hit')\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Reward', reward)\n",
    "            print('You won') if reward > 0 else print('You lost')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction of Action Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy is stick when sum>18 and hit when sum<18 with 80% probability for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(bj_env):\n",
    "    episode=[]\n",
    "    state= bj_env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        probs = [0.8,0.2] if state[0]>18 else [0.2,0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state= next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((13, 10, False), 1, -1)]\n",
      "[((9, 10, False), 1, 0), ((16, 10, False), 1, 0), ((18, 10, False), 1, -1)]\n",
      "[((15, 3, False), 0, -1.0)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_episode(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_val_pred(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    \n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        \n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        \n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        \n",
    "        # update the sum of the returns, number of visits, and action-value \n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            \n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            \n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = action_val_pred(env, 500, generate_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.action_val_pred.<locals>.<lambda>()>,\n",
       "            {(20, 3, False): array([ 0.75, -1.  ]),\n",
       "             (13, 10, False): array([-0.5       , -0.73684211]),\n",
       "             (17, 10, False): array([-0.25      , -0.33333333]),\n",
       "             (21, 10, False): array([ 0.72727273, -1.        ]),\n",
       "             (14, 1, False): array([ 0.        , -0.66666667]),\n",
       "             (8, 5, False): array([ 0., -1.]),\n",
       "             (17, 5, False): array([ 0., -1.]),\n",
       "             (17, 1, True): array([0., 0.]),\n",
       "             (19, 1, True): array([0., 0.]),\n",
       "             (21, 6, True): array([ 0., -1.]),\n",
       "             (16, 6, False): array([-1., -1.]),\n",
       "             (17, 7, False): array([ 0. , -0.2]),\n",
       "             (21, 7, False): array([1., 0.]),\n",
       "             (21, 1, True): array([ 0., -1.]),\n",
       "             (12, 1, False): array([-1. , -0.8]),\n",
       "             (16, 1, False): array([ 0., -1.]),\n",
       "             (17, 1, False): array([-1., -1.]),\n",
       "             (12, 2, True): array([ 0., -1.]),\n",
       "             (13, 2, True): array([-1.,  0.]),\n",
       "             (18, 10, False): array([-1.        , -0.88235294]),\n",
       "             (11, 5, False): array([ 0.  , -0.25]),\n",
       "             (19, 5, False): array([ 1., -1.]),\n",
       "             (14, 4, False): array([-1. , -0.5]),\n",
       "             (15, 4, False): array([ 0., -1.]),\n",
       "             (20, 2, True): array([0.4, 0. ]),\n",
       "             (9, 9, False): array([0., 1.]),\n",
       "             (11, 9, False): array([1., 0.]),\n",
       "             (20, 9, False): array([ 0.875, -1.   ]),\n",
       "             (9, 10, False): array([-0.33333333, -0.4       ]),\n",
       "             (16, 4, False): array([-1. , -0.8]),\n",
       "             (7, 4, False): array([ 0.        , -0.33333333]),\n",
       "             (17, 4, False): array([ 0. , -0.2]),\n",
       "             (21, 4, False): array([ 1., -1.]),\n",
       "             (15, 5, False): array([-1. , -0.6]),\n",
       "             (14, 1, True): array([ 0., -1.]),\n",
       "             (15, 9, False): array([ 1., -1.]),\n",
       "             (20, 10, False): array([ 0.42857143, -1.        ]),\n",
       "             (9, 8, False): array([ 0., -1.]),\n",
       "             (12, 8, False): array([0., 0.]),\n",
       "             (13, 4, False): array([ 1., -1.]),\n",
       "             (14, 10, False): array([ 0. , -0.5]),\n",
       "             (19, 2, False): array([ 0.2, -1. ]),\n",
       "             (4, 10, False): array([0., 0.]),\n",
       "             (15, 10, True): array([-1.,  0.]),\n",
       "             (8, 7, False): array([ 1., -1.]),\n",
       "             (17, 2, False): array([0. , 0.2]),\n",
       "             (20, 6, False): array([ 0.5, -1. ]),\n",
       "             (15, 3, False): array([0., 0.]),\n",
       "             (16, 3, False): array([ 1. , -0.6]),\n",
       "             (21, 2, True): array([ 0., -1.]),\n",
       "             (16, 2, False): array([-1.,  0.]),\n",
       "             (11, 10, False): array([-1. , -0.5]),\n",
       "             (19, 10, False): array([ 0., -1.]),\n",
       "             (14, 9, False): array([ 1.        , -0.66666667]),\n",
       "             (12, 2, False): array([-1.        , -0.33333333]),\n",
       "             (13, 6, False): array([-1. , -0.6]),\n",
       "             (6, 10, False): array([-1.        , -0.33333333]),\n",
       "             (16, 10, False): array([ 1. , -0.5]),\n",
       "             (12, 10, False): array([-1.        , -0.66666667]),\n",
       "             (12, 6, False): array([0.        , 0.33333333]),\n",
       "             (13, 3, True): array([1., 0.]),\n",
       "             (20, 10, True): array([1., 0.]),\n",
       "             (7, 6, False): array([1., 0.]),\n",
       "             (13, 5, False): array([0.        , 0.33333333]),\n",
       "             (13, 3, False): array([0., 0.]),\n",
       "             (15, 10, False): array([-1.        , -0.71428571]),\n",
       "             (21, 4, True): array([1., 0.]),\n",
       "             (10, 4, False): array([-1., -1.]),\n",
       "             (18, 4, False): array([ 0., -1.]),\n",
       "             (20, 2, False): array([1., 0.]),\n",
       "             (13, 8, False): array([1.        , 0.33333333]),\n",
       "             (20, 8, False): array([ 1., -1.]),\n",
       "             (17, 9, False): array([-0.33333333,  0.        ]),\n",
       "             (14, 5, False): array([-1. , -0.5]),\n",
       "             (21, 5, False): array([ 0.75, -1.  ]),\n",
       "             (16, 3, True): array([ 0., -1.]),\n",
       "             (19, 8, False): array([1., 0.]),\n",
       "             (10, 9, False): array([0., 1.]),\n",
       "             (19, 3, False): array([ 0.66666667, -1.        ]),\n",
       "             (17, 6, False): array([ 0., -1.]),\n",
       "             (12, 9, False): array([ 1. , -0.5]),\n",
       "             (16, 9, False): array([ 0.        , -0.57142857]),\n",
       "             (12, 3, False): array([0., 0.]),\n",
       "             (10, 10, False): array([ 0., -1.]),\n",
       "             (16, 7, False): array([ 0. , -0.5]),\n",
       "             (21, 8, True): array([1., 0.]),\n",
       "             (18, 5, False): array([ 0., -1.]),\n",
       "             (20, 5, False): array([ 0.33333333, -1.        ]),\n",
       "             (5, 9, False): array([ 0., -1.]),\n",
       "             (15, 2, False): array([ 0., -1.]),\n",
       "             (5, 5, False): array([0., 0.]),\n",
       "             (18, 2, False): array([ 0.5, -0.6]),\n",
       "             (13, 10, True): array([0., 0.]),\n",
       "             (16, 10, True): array([ 0., -1.]),\n",
       "             (15, 7, True): array([0., 1.]),\n",
       "             (14, 7, False): array([-1.,  0.]),\n",
       "             (20, 7, False): array([ 0.5, -1. ]),\n",
       "             (20, 4, False): array([ 0., -1.]),\n",
       "             (21, 3, True): array([0., 0.]),\n",
       "             (19, 6, False): array([0.5, 0. ]),\n",
       "             (14, 6, False): array([ 0.        , -0.33333333]),\n",
       "             (7, 10, False): array([ 0., -1.]),\n",
       "             (9, 2, False): array([ 0.        , -0.33333333]),\n",
       "             (12, 4, False): array([0., 0.]),\n",
       "             (16, 5, False): array([ 0.        , -0.66666667]),\n",
       "             (21, 2, False): array([1., 0.]),\n",
       "             (19, 9, False): array([0.33333333, 0.        ]),\n",
       "             (12, 5, False): array([-1.,  0.]),\n",
       "             (11, 2, False): array([ 0., -1.]),\n",
       "             (15, 1, False): array([ 0. , -0.5]),\n",
       "             (21, 1, False): array([ 0.66666667, -1.        ]),\n",
       "             (15, 9, True): array([ 0.        , -0.33333333]),\n",
       "             (16, 9, True): array([ 0., -1.]),\n",
       "             (17, 9, True): array([0., 0.]),\n",
       "             (5, 10, False): array([ 0., -1.]),\n",
       "             (18, 9, False): array([ 0., -1.]),\n",
       "             (18, 8, False): array([ 0., -1.]),\n",
       "             (12, 7, False): array([ 0. , -0.5]),\n",
       "             (14, 2, False): array([-1., -1.]),\n",
       "             (17, 3, True): array([0., 1.]),\n",
       "             (17, 3, False): array([ 0., -1.]),\n",
       "             (21, 10, True): array([ 0.83333333, -1.        ]),\n",
       "             (18, 3, True): array([0., 0.]),\n",
       "             (18, 1, False): array([ 1., -1.]),\n",
       "             (21, 9, True): array([1., 0.]),\n",
       "             (13, 7, False): array([-1., -1.]),\n",
       "             (5, 8, False): array([-1.,  1.]),\n",
       "             (10, 8, False): array([0., 1.]),\n",
       "             (14, 8, False): array([-1.        ,  0.33333333]),\n",
       "             (5, 2, False): array([ 0., -1.]),\n",
       "             (7, 2, False): array([0., 0.]),\n",
       "             (12, 8, True): array([-1.,  0.]),\n",
       "             (15, 7, False): array([-1.,  0.]),\n",
       "             (13, 2, False): array([ 0., -1.]),\n",
       "             (19, 1, False): array([ 1., -1.]),\n",
       "             (13, 1, False): array([ 0., -1.]),\n",
       "             (20, 1, False): array([0.75, 0.  ]),\n",
       "             (21, 6, False): array([1., 0.]),\n",
       "             (19, 10, True): array([-0.25,  1.  ]),\n",
       "             (8, 10, False): array([ 0. , -0.2]),\n",
       "             (13, 9, False): array([1.        , 0.33333333]),\n",
       "             (9, 3, False): array([-1. , -0.5]),\n",
       "             (19, 4, False): array([ 0., -1.]),\n",
       "             (4, 7, False): array([ 0., -1.]),\n",
       "             (9, 7, False): array([1., 0.]),\n",
       "             (21, 8, False): array([1., 0.]),\n",
       "             (14, 5, True): array([0., 1.]),\n",
       "             (16, 5, True): array([0. , 0.5]),\n",
       "             (19, 5, True): array([1., 0.]),\n",
       "             (15, 6, True): array([ 0., -1.]),\n",
       "             (15, 6, False): array([ 0., -1.]),\n",
       "             (10, 2, False): array([ 0., -1.]),\n",
       "             (10, 6, False): array([-1., -1.]),\n",
       "             (18, 6, False): array([-1., -1.]),\n",
       "             (5, 6, False): array([ 0., -1.]),\n",
       "             (11, 6, False): array([ 0., -1.]),\n",
       "             (9, 4, False): array([ 0., -1.]),\n",
       "             (17, 4, True): array([ 0., -1.]),\n",
       "             (10, 7, False): array([-1.,  0.]),\n",
       "             (21, 9, False): array([ 0., -1.]),\n",
       "             (18, 2, True): array([0., 1.]),\n",
       "             (8, 3, False): array([-1.,  0.]),\n",
       "             (13, 7, True): array([ 0., -1.]),\n",
       "             (4, 1, False): array([ 0., -1.]),\n",
       "             (6, 1, False): array([ 0., -1.]),\n",
       "             (10, 1, False): array([ 0., -1.]),\n",
       "             (21, 5, True): array([ 1., -1.]),\n",
       "             (15, 8, False): array([-1., -1.]),\n",
       "             (15, 8, True): array([ 0., -1.]),\n",
       "             (18, 7, False): array([ 1., -1.]),\n",
       "             (14, 10, True): array([ 0., -1.]),\n",
       "             (18, 1, True): array([ 0., -1.]),\n",
       "             (19, 8, True): array([1., 0.]),\n",
       "             (11, 3, False): array([0. , 0.5]),\n",
       "             (21, 3, False): array([0.66666667, 0.        ]),\n",
       "             (17, 8, False): array([ 1., -1.]),\n",
       "             (20, 7, True): array([1., 0.]),\n",
       "             (16, 8, False): array([ 0., -1.]),\n",
       "             (12, 10, True): array([-1.,  0.]),\n",
       "             (9, 6, False): array([1., 0.]),\n",
       "             (20, 9, True): array([ 1., -1.]),\n",
       "             (7, 7, False): array([ 0., -1.]),\n",
       "             (9, 1, False): array([-1.,  0.]),\n",
       "             (11, 4, False): array([ 0., -1.]),\n",
       "             (13, 5, True): array([0., 1.]),\n",
       "             (11, 7, False): array([ 0., -1.]),\n",
       "             (6, 4, False): array([ 0., -1.]),\n",
       "             (5, 3, False): array([-1.,  0.]),\n",
       "             (19, 7, False): array([1., 0.]),\n",
       "             (21, 7, True): array([1., 0.]),\n",
       "             (15, 5, True): array([0., 1.]),\n",
       "             (13, 9, True): array([ 0., -1.]),\n",
       "             (14, 9, True): array([ 0., -1.]),\n",
       "             (9, 5, False): array([ 0., -1.]),\n",
       "             (14, 3, False): array([-1.,  0.]),\n",
       "             (6, 3, False): array([ 0., -1.]),\n",
       "             (18, 5, True): array([0., 1.]),\n",
       "             (15, 2, True): array([0., 1.])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
